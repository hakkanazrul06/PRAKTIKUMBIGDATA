{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdde53c-6ade-47ff-8d6f-78c40283c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/23 20:05:38 WARN Utils: Your hostname, hakkan-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/23 20:05:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 20:05:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/23 20:05:59 WARN Instrumentation: [c28fd939] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/23 20:06:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/23 20:06:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5212e22a-d44b-4708-8a34-5ea013dec2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057964709234,4.087352278330043]\n",
      "Intercept: 11.568912762179835\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b16cccf5-d60f-417f-8c4a-3f5011fd1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0, 10.0])),\n",
    "    (4, Vectors.dense([15.0, 15.0]))\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20c791ba-c4ad-4e43-b672-51fe9d4b05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|\n",
      "| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|\n",
      "| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|\n",
      "| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|\n",
      "| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Homework 1\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df = spark.read.csv(\"/home/hakkan/Downloads/datasets/heart_disease.csv\", header=True, inferSchema=True)\n",
    "\n",
    "label = \"target\"\n",
    "categorical = [\"sex\", \"cp\", \"thal\", \"slope\"]\n",
    "numeric = [c for c in df.columns if c not in categorical + [label]]\n",
    "stages = []\n",
    "\n",
    "for c in categorical:\n",
    "    stages += [\n",
    "        StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\"),\n",
    "        OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\")\n",
    "    ]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[c+\"_vec\" for c in categorical] + numeric, outputCol=\"features\")\n",
    "\n",
    "stages.append(assembler)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df).select(\"features\", label)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d05490f-9c12-40de-b677-e065b0efe479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8402366863905325\n"
     ]
    }
   ],
   "source": [
    "#Homework 2\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_multi = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_multi.evaluate(pred)\n",
    "print(\"Accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56bbbdf1-2283-4839-bad4-0f780a841984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy = 0.8402366863905325\n",
      "Best regParam = 0.01\n",
      "Best maxIter  = 50\n"
     ]
    }
   ],
   "source": [
    "#Homework 3\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_multi = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1])\n",
    "             .addGrid(lr.maxIter, [50, 100])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_multi,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "bestPred = cvModel.transform(test)\n",
    "\n",
    "print(\"Best Accuracy =\", evaluator_multi.evaluate(bestPred))\n",
    "print(\"Best regParam =\", cvModel.bestModel._java_obj.getRegParam())\n",
    "print(\"Best maxIter  =\", cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bc6f1-1a14-47ee-8c0a-e5025dbb0d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
